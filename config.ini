[llm]
model = mistral
model_provider = ollama
temperature = 0.0
base_url = http://host.docker.internal:11434
max_tokens = 1000

[translation]
source_lang = auto
target_lang = en
use_cache = true
timeout = 5

[logging]
level = INFO
format=%%(asctime)s | %%(levelname)s | %%(name)s | %%(message)s